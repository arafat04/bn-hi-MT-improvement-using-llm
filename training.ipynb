{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arafat04/bn-hi-MT-improvement-using-llm/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E9qCD6zUMhF"
      },
      "source": [
        "## Prompt formatting utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIm3iLcPUMhI"
      },
      "source": [
        "Simple helper functions to format prompts later on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKiclSuOUMhJ"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"{source_lang}: {source_text}\n",
        "{target_lang}: {target_text}\"\"\"\n",
        "\n",
        "def apply_prompt(training=False, eos_token=None, **kwargs):\n",
        "    # note: we strip because of potential trailing whitespace\n",
        "    # we also provide a default value for target_text so that it can be omitted\n",
        "    return template.format(**{\"target_text\": \"\", **kwargs}).strip() + (\"\" if not training or eos_token is None else eos_token)\n",
        "\n",
        "def apply_prompt_n_shot(examples, n: int, eos_token: str, **kwargs):\n",
        "    return (eos_token + \"\\n\\n\").join(\n",
        "        [apply_prompt(**{\"target_text\": \"\", **example}) for example in examples[:n]] + [apply_prompt(**kwargs)]\n",
        "    )\n",
        "\n",
        "EXAMPLE_SENTENCES = [\n",
        "    {\n",
        "        \"source_lang\": \"Bengali\",\n",
        "        \"target_lang\": \"Hindi\",\n",
        "        \"source_text\": \"খবরটা শুনে খুব খারাপ লাগলো।\",\n",
        "        \"target_text\": \"खबर सुनकर बहुत दुख हुआ.\",\n",
        "    },\n",
        "    {\n",
        "        \"source_lang\": \"Bengali\",\n",
        "        \"target_lang\": \"Hindi\",\n",
        "        \"source_text\": \"এটার দাম কত?\",\n",
        "        \"target_text\": \"इसकी कीमत कितनी होती है?\",\n",
        "    },\n",
        "    {\n",
        "        \"source_lang\": \"Bengali\",\n",
        "        \"target_lang\": \"Hindi\",\n",
        "        \"source_text\": \"ঢাকা বাংলাদেশের রাজধানী।\",\n",
        "        \"target_text\": \"ढाका बांग्लादेश की राजधानी है.\",\n",
        "    },\n",
        "    {\n",
        "        \"source_lang\": \"Bengali\",\n",
        "        \"target_lang\": \"Hindi\",\n",
        "        \"source_text\": \"রাস্তার দিকে মনোযোগ দিন।\",\n",
        "        \"target_text\": \"सड़क पर ध्यान दें.\",\n",
        "    },\n",
        "    {\n",
        "        \"source_lang\": \"Bengali\",\n",
        "        \"target_lang\": \"Hindi\",\n",
        "        \"source_text\": \"আমার মাথা ব্যথা করছে\",\n",
        "        \"target_text\": \"मुझे सिर दर्द है.\",\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBglVOU5UMhM",
        "outputId": "c938faa9-12af-43b1-97fe-2c9a38138e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: হ্যালো.\n",
            "Hindi:\n"
          ]
        }
      ],
      "source": [
        "print(apply_prompt(source_lang=\"Bengali\", source_text=\"হ্যালো.\", target_lang=\"Hindi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS8iyUV0UMhO",
        "outputId": "bf37e2f8-3016-48ee-85de-4ce151788a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: হ্যালো.\n",
            "Hindi: नमस्ते.</s>\n"
          ]
        }
      ],
      "source": [
        "print(apply_prompt(source_lang=\"Bengali\", source_text=\"হ্যালো.\", target_lang=\"Hindi\", target_text=\"नमस्ते.\", eos_token=\"</s>\", training=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykBHRMEkUMhO",
        "outputId": "e7656a30-d6fb-4180-89fc-5fa47e623748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: খবরটা শুনে খুব খারাপ লাগলো।\n",
            "Hindi: खबर सुनकर बहुत दुख हुआ.</s>\n",
            "\n",
            "Bengali: এটার দাম কত?\n",
            "Hindi: इसकी कीमत कितनी होती है?</s>\n",
            "\n",
            "Bengali: ঢাকা বাংলাদেশের রাজধানী।\n",
            "Hindi: ढाका बांग्लादेश की राजधानी है.</s>\n",
            "\n",
            "Bengali: রাস্তার দিকে মনোযোগ দিন।\n",
            "Hindi: सड़क पर ध्यान दें.</s>\n",
            "\n",
            "Bengali: আমার মাথা ব্যথা করছে\n",
            "Hindi: मुझे सिर दर्द है.</s>\n",
            "\n",
            "Bengali: হ্যালো.\n",
            "Hindi:\n"
          ]
        }
      ],
      "source": [
        "print(apply_prompt_n_shot(EXAMPLE_SENTENCES, 5, \"</s>\", source_lang=\"Bengali\", source_text=\"হ্যালো.\", target_lang=\"Hindi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIVnPxXsUMhP",
        "outputId": "98e927db-7c98-4f6a-9e94-acaa3f2386e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: খবরটা শুনে খুব খারাপ লাগলো।\n",
            "Hindi: खबर सुनकर बहुत दुख हुआ.</s>\n",
            "\n",
            "Bengali: এটার দাম কত?\n",
            "Hindi: इसकी कीमत कितनी होती है?</s>\n",
            "\n",
            "Bengali: ঢাকা বাংলাদেশের রাজধানী।\n",
            "Hindi: ढाका बांग्लादेश की राजधानी है.</s>\n",
            "\n",
            "Bengali: রাস্তার দিকে মনোযোগ দিন।\n",
            "Hindi: सड़क पर ध्यान दें.</s>\n",
            "\n",
            "Bengali: আমার মাথা ব্যথা করছে\n",
            "Hindi: मुझे सिर दर्द है.</s>\n",
            "\n",
            "Bengali: হ্যালো.\n",
            "Hindi: नमस्ते.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(apply_prompt_n_shot(EXAMPLE_SENTENCES, 5, \"</s>\", source_lang=\"Bengali\", source_text=\"হ্যালো.\", target_lang=\"Hindi\", target_text=\"नमस्ते.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iWZ_411UMhP",
        "outputId": "5b741c6f-8d93-4fb8-ebea-433ed53aa38b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/storage/praha1/home/rahmang/envs/unsloth/lib/python3.10/site-packages/unsloth/__init__.py:22: UserWarning: Unsloth: 'CUDA_VISIBLE_DEVICES' is currently GPU-044e48b9-e794-cbfc-316e-1472ca268b72 but we require 'CUDA_VISIBLE_DEVICES=0'\n",
            "We shall set it ourselves.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdgbGGPzUMhQ"
      },
      "source": [
        "## Load the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8CDwB_PUMhQ",
        "outputId": "99bb5482-7285-4cf9-dc4e-54d38c8a8f18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/storage/praha1/home/rahmang/envs/unsloth/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Mistral patching release 2024.3\n",
            "   \\\\   /|    GPU: NVIDIA RTX A4000. Max memory: 15.724 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/storage/praha1/home/rahmang/envs/unsloth/lib/python3.10/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-bnb-4bit\",\n",
        "    max_seq_length=4096,\n",
        "    load_in_4bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYLK3gyxUMhR",
        "outputId": "2daa237a-368d-4db1-f9b9-0822a145fa2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\n",
            "Hindi:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<s> Bengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\\nHindi: प्रशिक्षण अलगोरिडम, परामिति, न्यायता सीमाबद्धता वा अन्यान्य प्रयोग पद्धति एवं वैशिष्ट्य संपर्के तथ्य।\\n\\n## Definition of Algorithm\\n\\nAn algorithm is a finite set of well-defined instructions for solving a problem.\\n']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "sentence = \"প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\"\n",
        "prompt = apply_prompt(source_lang=\"Bengali\", source_text=sentence, target_lang=\"Hindi\")\n",
        "print(prompt)\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQC3LYecUMhS",
        "outputId": "e92691fe-9eae-4f54-99ec-cbab783644ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: খবরটা শুনে খুব খারাপ লাগলো।\n",
            "Hindi: खबर सुनकर बहुत दुख हुआ.</s>\n",
            "\n",
            "Bengali: এটার দাম কত?\n",
            "Hindi: इसकी कीमत कितनी होती है?</s>\n",
            "\n",
            "Bengali: ঢাকা বাংলাদেশের রাজধানী।\n",
            "Hindi: ढाका बांग्लादेश की राजधानी है.</s>\n",
            "\n",
            "Bengali: রাস্তার দিকে মনোযোগ দিন।\n",
            "Hindi: सड़क पर ध्यान दें.</s>\n",
            "\n",
            "Bengali: আমার মাথা ব্যথা করছে\n",
            "Hindi: मुझे सिर दर्द है.</s>\n",
            "\n",
            "Bengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\n",
            "Hindi:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<s> Bengali: খবরটা শুনে খুব খারাপ লাগলো।\\nHindi: खबर सुनकर बहुत दुख हुआ.</s>\\n\\nBengali: এটার দাম কত?\\nHindi: इसकी कीमत कितनी होती है?</s>\\n\\nBengali: ঢাকা বাংলাদেশের রাজধানী।\\nHindi: ढाका बांग्लादेश की राजधानी है.</s>\\n\\nBengali: রাস্তার দিকে মনোযোগ দিন।\\nHindi: सड़क पर ध्यान दें.</s>\\n\\nBengali: আমার মাথা ব্যথা করছে\\nHindi: मुझे सिर दर्द है.</s>\\n\\nBengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\\nHindi: प्रशिक्षण अलगोरिडम, परामिति, न्यायता सीमाबद्धता वा अन्यान्य प्रयोग पद्धति एवं वैशिष्ट्य संपर्क की जानकारी। industries\\n\\nBengali: খবরটা শুনে খুব খারাপ লাগলো।\\nHindi: खबर सुनकर बहुत दुख हुआ. industries\\n\\nBengali: এটার দাম কত?\\nHindi: इसकी कीमत कितनी होती है? industries\\n\\nBengali: ঢাকা বাংলাদেশের রাজ��']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = apply_prompt_n_shot(EXAMPLE_SENTENCES, 5, eos_token=tokenizer.eos_token, source_lang=\"Bengali\", source_text=sentence, target_lang=\"Hindi\")\n",
        "print(prompt)\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EvgNyMHUMhS"
      },
      "source": [
        "## LoRa Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meCSk7sgUMhT",
        "outputId": "17cd3f6a-1bcb-44d5-ff23-477129e2dadc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0,  # 0 is optimized (unsloth)\n",
        "    bias=\"none\",  # \"none\" is optimized (unsloth)\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=42,\n",
        "    use_rslora=False,  # rank stabilized LoRA\n",
        "    loftq_config=None,  # LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYFxTc1UMhT"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s3IAT670UMhU"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# def formatting_prompts_func(examples):\n",
        "#     # dict of lists to list of dicts\n",
        "#     examples = [dict(zip(examples, t)) for t in zip(*examples.values())]\n",
        "#     texts = []\n",
        "#     for example in examples:\n",
        "#         text = (\n",
        "#             apply_prompt(training=True, eos_token=tokenizer.eos_token, **example)\n",
        "#             # + EOS_TOKEN  # FIXME: remove from apply_prompt and do here explicitly\n",
        "#         )\n",
        "#         texts.append(text)\n",
        "#     return {\n",
        "#         \"text\": texts,\n",
        "#     }\n",
        "# name = \"bn-hi-parallel-data\"\n",
        "# #dataset_path = f\"/storage/praha1/home/rahmang/{name}\"\n",
        "# dataset_path = \"grarafat/test-bn-hi-parallel-data\"\n",
        "# # dataset_path = \"~/datasets/npfl087-demo-small\"\n",
        "# dataset = load_dataset(dataset_path, split=\"train\", token=\"give the access token from huggingface\")\n",
        "# dataset = dataset.map(\n",
        "#     formatting_prompts_func,\n",
        "#     batched=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myuMM_zcUMhU",
        "outputId": "515a10d3-9c7f-4021-83b4-7edc85b6e594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_lang target_lang                                        source_text  \\\n",
            "0     Bengali       Hindi  ইকুয়েডরীয় গ্যালোপেগোস দ্বীপপুঞ্জের এক নতুন প্র...   \n",
            "1     Bengali       Hindi  মার্কিন যুক্তরাষ্ট্রের প্রিন্সটন বিশ্ববিদ্যালয...   \n",
            "2     Bengali       Hindi  \"এই বিতর্কটি হারিকেন ক্যাটরিনার সময় ত্রাণ ও পু...   \n",
            "3     Bengali       Hindi  পুনর্নির্মাণের চেষ্টা সম্পর্কে উদার সমালোচনা ও...   \n",
            "4     Bengali       Hindi  খেলাটি দুর্দান্ত আবহাওয়ার সাথে সকাল ১০:০০ টায...   \n",
            "\n",
            "                                         target_text  \\\n",
            "0  जर्नल साइंस में गुरूवार को प्रकाशित एक अध्ययन ...   \n",
            "1  संयुक्त राज्य अमेरिका में प्रिंसटन विश्वविद्या...   \n",
            "2  तूफ़ान कैटरीना के मद्देनज़र राहत और पुनर्निर्म...   \n",
            "3  पुनर्निर्माण की कोशिश की स्पष्ट आलोचना वॉशिंगट...   \n",
            "4  खेल सुबह 10:00 बजे बहुत बढ़िया मौसम के साथ शुर...   \n",
            "\n",
            "                                                text  \n",
            "0  Bengali: ইকুয়েডরীয় গ্যালোপেগোস দ্বীপপুঞ্জের এক...  \n",
            "1  Bengali: মার্কিন যুক্তরাষ্ট্রের প্রিন্সটন বিশ্...  \n",
            "2  Bengali: \"এই বিতর্কটি হারিকেন ক্যাটরিনার সময় ত...  \n",
            "3  Bengali: পুনর্নির্মাণের চেষ্টা সম্পর্কে উদার স...  \n",
            "4  Bengali: খেলাটি দুর্দান্ত আবহাওয়ার সাথে সকাল ...  \n"
          ]
        }
      ],
      "source": [
        "# Select the first 5 rows\n",
        "first_five = dataset.select(range(5))\n",
        "\n",
        "# Convert to pandas DataFrame and display\n",
        "df = first_five.to_pandas()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Oyta5NUMhV",
        "outputId": "17626348-f86e-4109-b349-b06c93b88112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(503, 5)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lusXPvPTUMhV"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hETr0IlPUMhV"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=4096,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,  # Unsloth claim: \"can make training 5x faster for short sequences.\"\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        num_train_epochs=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cX05wroUMhW",
        "outputId": "44b83f6a-9256-4715-9fc8-ec236c81e8b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in ./envs/unsloth/lib/python3.10/site-packages (0.16.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (5.9.8)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (2.0.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (69.5.1)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in ./envs/unsloth/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
            "Requirement already satisfied: six>=1.4.0 in ./envs/unsloth/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in ./envs/unsloth/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./envs/unsloth/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./envs/unsloth/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./envs/unsloth/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./envs/unsloth/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in ./envs/unsloth/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZYnUed5UMhW",
        "outputId": "3f1b79fc-be65-4b85-8d21-6ba7ac6e5cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!wandb.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X-5HhHAmUMhW"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import wandb\n",
        "\n",
        "# # Set your Weights and Biases API key as an environment variable\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"give the access api\"\n",
        "\n",
        "# # Log in to Weights and Biases using the API key\n",
        "# wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTXqTiPNUMhX",
        "outputId": "69bb8c24-5b2e-4f93-c403-110f24e0a79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA RTX A4000. Max memory = 15.724 GB.\n",
            "4.789 GB of memory reserved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 35 | Num Epochs = 2\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 8\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.18.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/auto/vestec1-elixir/home/rahmang/wandb/run-20241128_211749-d4gr7r2n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/grarafat/huggingface/runs/d4gr7r2n' target=\"_blank\">hardy-capybara-2</a></strong> to <a href='https://wandb.ai/grarafat/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/grarafat/huggingface' target=\"_blank\">https://wandb.ai/grarafat/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/grarafat/huggingface/runs/d4gr7r2n' target=\"_blank\">https://wandb.ai/grarafat/huggingface/runs/d4gr7r2n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 03:58, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.587700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.708000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.526700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.414600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.228000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.168100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.100200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "283.7293 seconds used for training.\n",
            "4.73 minutes used for training.\n",
            "Peak reserved memory = 9.367 GB.\n",
            "Peak reserved memory % of max memory = 59.571 %.\n"
          ]
        }
      ],
      "source": [
        "# These stats are copied from the unsloth colab example notebook\n",
        "# Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5JExl8JUMhX"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn9ohNUQUMhY",
        "outputId": "b507455f-b29e-446e-efec-12a9484c4418"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/storage/praha1/home/rahmang/envs/unsloth/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"outputs/mistral-ft-qlora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJWn6Xb_UMhY"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQrmmLMJUMhY",
        "outputId": "b69a64f4-e3c4-4690-e5db-025632f560c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\n",
            "Hindi:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<s> Bengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\\nHindi: प्रशिक्षण अल्गोरिदम, परमिति, न्यायता सीमाबद्धता वा अन्यान्य प्रयोग पद्धति और विषयी का जानकारी।\\n\\n## Overview\\n\\nThe Algorithmic Fairness and Accountability Toolkit (AFA Toolkit) is a collection of resources for researchers, practitioners, and policymakers to understand and address algorithmic fairness and accountability. The AFA Toolkit is a living document that will be updated as new resources become available.\\n\\nThe AFA Toolkit is organized into three sections:\\n\\n- Algorithmic Fairness and Accountability: This section provides an overview of algorithmic fairness and accountability, including definitions, examples, and resources for further reading.\\n- Algorithmic Fairness and Accountability in Practice: This section provides practical guidance for implementing algorithmic fairness and accountability in practice, including best practices, case studies, and resources']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "sentence = \"প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\"\n",
        "prompt = apply_prompt(source_lang=\"Bengali\", source_text=sentence, target_lang=\"Hindi\")\n",
        "print(prompt)\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytrxvOrXUMhZ",
        "outputId": "199f0e65-381f-468e-d9d8-9469bedc473c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bengali: খবরটা শুনে খুব খারাপ লাগলো।\n",
            "Hindi: खबर सुनकर बहुत दुख हुआ.</s>\n",
            "\n",
            "Bengali: এটার দাম কত?\n",
            "Hindi: इसकी कीमत कितनी होती है?</s>\n",
            "\n",
            "Bengali: ঢাকা বাংলাদেশের রাজধানী।\n",
            "Hindi: ढाका बांग्लादेश की राजधानी है.</s>\n",
            "\n",
            "Bengali: রাস্তার দিকে মনোযোগ দিন।\n",
            "Hindi: सड़क पर ध्यान दें.</s>\n",
            "\n",
            "Bengali: আমার মাথা ব্যথা করছে\n",
            "Hindi: मुझे सिर दर्द है.</s>\n",
            "\n",
            "Bengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\n",
            "Hindi:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<s> Bengali: খবরটা শুনে খুব খারাপ লাগলো।\\nHindi: खबर सुनकर बहुत दुख हुआ.</s>\\n\\nBengali: এটার দাম কত?\\nHindi: इसकी कीमत कितनी होती है?</s>\\n\\nBengali: ঢাকা বাংলাদেশের রাজধানী।\\nHindi: ढाका बांग्लादेश की राजधानी है.</s>\\n\\nBengali: রাস্তার দিকে মনোযোগ দিন।\\nHindi: सड़क पर ध्यान दें.</s>\\n\\nBengali: আমার মাথা ব্যথা করছে\\nHindi: मुझे सिर दर्द है.</s>\\n\\nBengali: প্রশিক্ষণ অ্যালগরিদম, পরামিতি, ন্যায্যতা সীমাবদ্ধতা বা অন্যান্য প্রয়োগ পদ্ধতি এবং বৈশিষ্ট্য সম্পর্কে তথ্য।\\nHindi: प्रशिक्षण अल्गोरिदम, परमीति, न्यायता सीमाबद्धता या अन्य प्रयोग पद्धति और विशिष्टता के बारे में जानकारी.</s>']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = apply_prompt_n_shot(EXAMPLE_SENTENCES, 5, eos_token=tokenizer.eos_token, source_lang=\"Bengali\", source_text=sentence, target_lang=\"Hindi\")\n",
        "print(prompt)\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}